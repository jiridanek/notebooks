# inspired by
# https://github.com/thesuperzapper/kubeflow/blob/master/.github/workflows/example_notebook_servers_publish_TEMPLATE.yaml
---
name: Build & Publish Notebook Servers (TEMPLATE)
"on":
  workflow_call:
    inputs:
      # https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables
      # https://docs.github.com/en/actions/learn-github-actions/contexts
      target:
        required: true
        description: "make target to build"
        type: string
      github:
        required: true
        description: "top workflow's `github`"
        type: string

jobs:
  build:
    runs-on: ubuntu-22.04
    env:
      # Use the rootful instance of podman for sharing images with cri-o
      # https://podman-desktop.io/blog/sharing-podman-images-with-kubernetes-cluster#introduction
      # https://access.redhat.com/solutions/6986565
      CONTAINER_HOST: unix:///var/run/podman/podman.sock
      # We don't push here when building PRs, so we can use the same IMAGE_REGISTRY in all branches of the workflow
      IMAGE_REGISTRY: "ghcr.io/${{ github.repository }}/workbench-images"
      # GitHub image registry used for storing $(CONTAINER_ENGINE)'s cache
      CACHE: "ghcr.io/${{ github.repository }}/workbench-images/build-cache"
      # Targets (and their folder) that should be scanned using FS instead of IMAGE scan due to resource constraints
      TRIVY_SCAN_FS_JSON: '{}'

    steps:

      - uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Free up additional disk space
        # https://docs.github.com/en/actions/learn-github-actions/expressions
        if: "${{ contains(inputs.target, 'rocm') || contains(inputs.target, 'cuda') || contains(inputs.target, 'intel') ||
         contains(inputs.target, 'pytorch') || contains(inputs.target, 'tensorflow') }}"
        run: |
          set -x

          df -h

          sudo apt-get update
          sudo apt-get remove -y '^dotnet-.*'
          sudo apt-get remove -y '^llvm-.*'
          sudo apt-get remove -y 'php.*'
          sudo apt-get remove -y '^mongodb-.*'
          sudo apt-get autoremove -y
          sudo apt-get clean
          sudo rm -rf /usr/local/.ghcup &
          sudo rm -rf /usr/local/lib/android &
          sudo rm -rf /usr/local/share/boost &
          sudo rm -rf /usr/local/lib/node_modules &
          sudo rm -rf /usr/share/dotnet &
          sudo rm -rf /opt/ghc &
          sudo rm -rf /opt/hostedtoolcache/CodeQL &

          sudo docker image prune --all --force &

          wait

          df -h

      - name: Mount lvm overlay for podman builds
        run: |
          df -h
          free -h

          bash ./ci/cached-builds/gha_lvm_overlay.sh

          df -h
          free -h

      # https://github.com/containers/buildah/issues/2521#issuecomment-884779112
      - name: Workaround https://github.com/containers/podman/issues/22152#issuecomment-2027705598
        run: sudo apt-get -qq remove podman crun

      - uses: actions/cache@v4
        id: cached-linuxbrew
        with:
          path: /home/linuxbrew/.linuxbrew
          key: linuxbrew

      - name: Install podman
        if: steps.cached-linuxbrew.outputs.cache-hit != 'true'
        run: |
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
          /home/linuxbrew/.linuxbrew/bin/brew install podman

      - name: Add linuxbrew to PATH
        run: echo "/home/linuxbrew/.linuxbrew/bin/" >> $GITHUB_PATH

      - name: Configure Podman
        run: |
          set -Eeuxo pipefail
          
          # podman from brew has its own /etc
          # the (default) config location is also where cri-o gets its storage defaults (that can be overriden in crio.conf)
          sudo cp ci/cached-builds/containers.conf /etc/containers.conf
          sudo cp ci/cached-builds/containers.conf /home/linuxbrew/.linuxbrew/opt/podman/etc/containers.conf
          sudo cp ci/cached-builds/storage.conf /etc/containers/storage.conf
          sudo cp ci/cached-builds/storage.conf /home/linuxbrew/.linuxbrew/opt/podman/etc/containers/storage.conf
          sudo cp ci/cached-builds/registries.conf /etc/containers/registries.conf
          sudo cp ci/cached-builds/registries.conf /home/linuxbrew/.linuxbrew/opt/podman/etc/containers/registries.conf

          # should at least reset storage when touching storage.conf
          # Failed to obtain podman configuration: runroot must be set
          mkdir -p $HOME/.local/share/containers/storage/tmp
          # remote (CONTAINER_HOST) podman does not do reset (and refuses --force option)
          podman info || true
          sudo podman info || true
          ls /home/linuxbrew/.linuxbrew/opt/podman/
          sudo CONTAINER_HOST= /home/linuxbrew/.linuxbrew/opt/podman/bin/podman system reset

          # https://github.com/containers/podman/blob/main/docs/tutorials/socket_activation.md
          # since `brew services start podman` is buggy, let's do our own brew-compatible service
          # Regarding directory paths, see https://unix.stackexchange.com/questions/224992/where-do-i-put-my-systemd-unit-file
          sudo mkdir -p /usr/local/lib/systemd/system/
          sudo cp ci/cached-builds/podman.service /usr/local/lib/systemd/system/podman.service
          sudo cp ci/cached-builds/podman.socket /usr/local/lib/systemd/system/podman.socket
          sudo systemctl daemon-reload
          sudo systemctl unmask --now podman.service podman.socket
          sudo systemctl start podman.socket

          # quick check podman works
          podman ps
      - name: Show error logs (on failure)
        if: ${{ failure() }}
        run: journalctl -xe

      - name: Calculate image name and tag
        id: calculated_vars
        run: |
          SANITIZED_REF_NAME=$(echo "${{ github.ref_name }}" | sed 's/[^a-zA-Z0-9._-]/_/g')
          IMAGE_TAG="${SANITIZED_REF_NAME}_${{ github.sha }}"
          
          echo "IMAGE_TAG=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#push
      - name: "push|schedule: make ${{ inputs.target }}"
        run: |
          
          make ${{ inputs.target }}
        if: ${{ fromJson(inputs.github).event_name == 'push' || fromJson(inputs.github).event_name == 'schedule' }}
        env:
          IMAGE_TAG: "${{ steps.calculated_vars.outputs.IMAGE_TAG }}"
          CONTAINER_BUILD_CACHE_ARGS: "--cache-from ${{ env.CACHE }} --cache-to ${{ env.CACHE }}"
          # dependent images were already built and pushed, so just let podman pull it
          BUILD_DEPENDENT_IMAGES: "no"

      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request
      - name: "pull_request: make ${{ inputs.target }}"
        run: |
          make ${{ inputs.target }}
        if: "${{ fromJson(inputs.github).event_name == 'pull_request' }}"
        env:
          IMAGE_TAG: "${{ steps.calculated_vars.outputs.IMAGE_TAG }}"
          CONTAINER_BUILD_CACHE_ARGS: "--cache-from ${{ env.CACHE }}"
          # We don't have access to image registry, so disable pushing
          PUSH_IMAGES: "no"

      - name: "Show podman images information"
        run: podman images --digests

      - name: "Check if we have tests or not"
        id: have-tests
#        run: "ci/cached-builds/has_tests.py --target ${{ inputs.target }}"
        run: echo "tests=true" >> "$GITHUB_OUTPUT"

      - run: ip addr ls

      # https://cri-o.io/
      - name: Install cri-o
        if: ${{ steps.have-tests.outputs.tests == 'true' }}
        run: |
          set -Eeuxo pipefail

          sudo apt-get update
          sudo apt-get install -y software-properties-common curl

          curl -fsSL https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/Release.key | \
            sudo gpg --dearmor --batch --yes -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

          echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/ /" | \
            sudo tee /etc/apt/sources.list.d/kubernetes.list

          curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/Release.key | \
            sudo gpg --dearmor --batch --yes -o /etc/apt/keyrings/cri-o-apt-keyring.gpg

          echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/ /" | \
            sudo tee /etc/apt/sources.list.d/cri-o.list

          sudo apt-get update
          sudo apt-get install -y cri-o kubelet kubeadm kubectl
          
          sudo cp ci/cached-builds/crio.conf /etc/crio/crio.conf.d/
          sudo rm -rf /etc/cni/net.d/11-crio-ipv4-bridge.conflist
          
          sudo systemctl start crio.service
        env:
          CRIO_VERSION: v1.30
          KUBERNETES_VERSION: v1.30

      - name: Show crio debug data (on failure)
        if: ${{ failure() }}
        run: |
          set -Eeuxo pipefail

          sudo systemctl status crio.service || true
          sudo journalctl -xeu crio.service

      # do this early, it's a good check that cri-o is not completely broken
      - name: "Show crio images information"
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: sudo crictl images

      - name: Install Kubernetes cluster
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: |
          set -Eeuxo pipefail

          sudo swapoff -a
          sudo modprobe br_netfilter
          sudo sysctl -w net.ipv4.ip_forward=1
          
          # https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm
          sudo kubeadm init --config=ci/cached-builds/kubeadm.yaml
          
          mkdir -p $HOME/.kube
          sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $(id -u):$(id -g) $HOME/.kube/config

      - name: Show kubelet debug data (on failure)
        if: ${{ failure() }}
        run: |
          set -Eeuxo pipefail

          sudo systemctl status kubelet || true
          sudo journalctl -xeu kubelet
          
          # Here is one example how you may list all running Kubernetes containers by using crictl:
          sudo crictl --runtime-endpoint unix:///var/run/crio/crio.sock ps -a | grep kube | grep -v pause
          # Once you have found the failing container, you can inspect its logs with:
          # crictl --runtime-endpoint unix:///var/run/crio/crio.sock logs CONTAINERID

      - name: Untaint the master
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: kubectl taint nodes --all node-role.kubernetes.io/control-plane-

      - name: Install pod network
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: kubectl apply -f https://github.com/flannel-io/flannel/releases/download/$FLANNEL_VERSION/kube-flannel.yml
        env:
          FLANNEL_VERSION: v0.26.1

      - name: Wait for pods to be running
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: |
          kubectl wait pods --all --all-namespaces --for=condition=Ready --timeout=300s

      # Configure a mutating webhook that simulates restricted-v2 using Kyverno
      # https://kyverno.io/docs/installation/methods/#install-kyverno-using-yamls
      - name: Install Kyverno
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: |
          set -Eeuxo pipefail

          kubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.11.1/install.yaml
          kubectl wait deployment --all --namespace kyverno --for=condition=Available --timeout=100s

          # this can fail if webhook is not yet ready to answer, retry is needed
          timeout 60s bash -c 'until kubectl create -f ci/cached-builds/kyverno.yaml; do sleep 1; done'
          # wait for policy to be in effect
          kubectl wait --for=condition=Ready clusterpolicy --all

      - name: Show debug data (on failure)
        if: ${{ failure() }}
        run: |
          set -Eeuxo pipefail

          kubectl describe deployments --namespace kyverno
          # deployments don't tell me why pods are failing
          kubectl describe pods --namespace kyverno
          kubectl logs deployments/kyverno-admission-controller --namespace kyverno

      - name: Show nodes status and wait for readiness
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
        run: |
          kubectl describe nodes
          kubectl wait --for=condition=Ready nodes --all --timeout=100s || (kubectl describe nodes && false)

      - name: "Run image tests"
        if: ${{ steps.have-tests.outputs.tests == 'true'  }}
#        run: python3 ci/cached-builds/make_test.py --target ${{ inputs.target }}
        run: python3 ci/cached-builds/make_test.py --target jupyter-minimal-ubi9-python-3.11
        env:
#          IMAGE_TAG: "${{ steps.calculated_vars.outputs.IMAGE_TAG }}"
          IMAGE_TAG: main_efe90c2ca935875bc6aa5b7653c42c77d14b419b
          IMAGE_REGISTRY: ghcr.io/jiridanek/notebooks/workbench-images
      - run: df -h
        if: "${{ !cancelled() }}"
