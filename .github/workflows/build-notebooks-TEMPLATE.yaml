# inspired by
# https://github.com/thesuperzapper/kubeflow/blob/master/.github/workflows/example_notebook_servers_publish_TEMPLATE.yaml
---
name: Build & Publish Notebook Servers (TEMPLATE)
"on":
  workflow_call:
    inputs:
      # https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables
      # https://docs.github.com/en/actions/learn-github-actions/contexts
      target:
        required: true
        description: "make target to build"
        type: string
      github:
        required: true
        description: "top workflow's `github`"
        type: string

jobs:
  build:
    runs-on: ubuntu-22.04
    env:
      # GitHub image registry used for storing $(CONTAINER_ENGINE)'s cache
      CACHE: "ghcr.io/${{ github.repository }}/workbench-images/build-cache"
      # Targets (and their folder) that should be scanned using FS instead of IMAGE scan due to resource constraints
      TRIVY_SCAN_FS_JSON: '{}'

    steps:

      - uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Free up additional disk space
        # https://docs.github.com/en/actions/learn-github-actions/expressions
        if: "${{ contains(inputs.target, 'rocm') || contains(inputs.target, 'cuda') || contains(inputs.target, 'intel') ||
         contains(inputs.target, 'pytorch') || contains(inputs.target, 'tensorflow') }}"
        run: |
          set -x

          df -h

          sudo apt-get update
          sudo apt-get remove -y '^dotnet-.*'
          sudo apt-get remove -y '^llvm-.*'
          sudo apt-get remove -y 'php.*'
          sudo apt-get remove -y '^mongodb-.*'
          sudo apt-get autoremove -y
          sudo apt-get clean
          sudo rm -rf /usr/local/.ghcup &
          sudo rm -rf /usr/local/lib/android &
          sudo rm -rf /usr/local/share/boost &
          sudo rm -rf /usr/local/lib/node_modules &
          sudo rm -rf /usr/share/dotnet &
          sudo rm -rf /opt/ghc &
          sudo rm -rf /opt/hostedtoolcache/CodeQL &

          sudo docker image prune --all --force &

          wait

          df -h

      - name: Mount lvm overlay for podman builds
        run: |
          df -h
          free -h

          bash ./ci/cached-builds/gha_lvm_overlay.sh

          df -h
          free -h

      # https://github.com/containers/buildah/issues/2521#issuecomment-884779112
      - name: Workaround https://github.com/containers/podman/issues/22152#issuecomment-2027705598
        run: sudo apt-get -qq remove podman crun

      - uses: actions/cache@v4
        id: cached-linuxbrew
        with:
          path: /home/linuxbrew/.linuxbrew
          key: linuxbrew

      - name: Install podman
        if: steps.cached-linuxbrew.outputs.cache-hit != 'true'
        run: |
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
          /home/linuxbrew/.linuxbrew/bin/brew install podman

      - name: Add linuxbrew to PATH
        run: echo "/home/linuxbrew/.linuxbrew/bin/" >> $GITHUB_PATH

      - name: Configure Podman
        run: |
          set -x
          mkdir -p $HOME/.config/containers/
          cp ci/cached-builds/containers.conf $HOME/.config/containers/containers.conf
          cp ci/cached-builds/storage.conf $HOME/.config/containers/storage.conf

          # should at least reset storage when touching storage.conf
          podman system reset --force
          mkdir -p $HOME/.local/share/containers/storage/tmp

          # configure for rootless KinD
          # https://kind.sigs.k8s.io/docs/user/rootless/
          sudo mkdir -p /etc/systemd/system/user@.service.d
          sudo cp ci/cached-builds/delegate.conf /etc/systemd/system/user@.service.d/delegate.conf
          sudo systemctl daemon-reload

          # start systemd user service
          # since `brew services start podman` is buggy, let's do our own brew-compatible service
          mkdir -p "${HOME}/.config/systemd/user/"
          cp ci/cached-builds/homebrew.podman.service "${HOME}/.config/systemd/user/homebrew.podman.service"
          systemctl --user daemon-reload
          systemctl --user start homebrew.podman.service
          echo "PODMAN_SOCK=/run/user/${UID}/podman/podman.sock" >> $GITHUB_ENV

      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#push
      - name: "push|schedule: make ${{ inputs.target }}"
        run: |
          SANITIZED_REF_NAME=$(echo "${{ github.ref_name }}" | sed 's/[^a-zA-Z0-9._-]/_/g')
          export IMAGE_TAG="${SANITIZED_REF_NAME}_${{ github.sha }}"
          make ${{ inputs.target }}
        if: ${{ fromJson(inputs.github).event_name == 'push' || fromJson(inputs.github).event_name == 'schedule' }}
        env:
          IMAGE_REGISTRY: "ghcr.io/${{ github.repository }}/workbench-images"
          CONTAINER_BUILD_CACHE_ARGS: "--cache-from ${{ env.CACHE }} --cache-to ${{ env.CACHE }}"
          # dependent images were already built and pushed, so just let podman pull it
          BUILD_DEPENDENT_IMAGES: "no"

      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request
      - name: "pull_request: make ${{ inputs.target }}"
        run: |
          make ${{ inputs.target }}
        if: "${{ fromJson(inputs.github).event_name == 'pull_request' }}"
        env:
          IMAGE_TAG: "${{ github.sha }}"
          IMAGE_REGISTRY: "ghcr.io/${{ github.repository }}/workbench-images"
          CONTAINER_BUILD_CACHE_ARGS: "--cache-from ${{ env.CACHE }}"
          # We don't have access to image registry, so disable pushing
          PUSH_IMAGES: "no"

      - name: "Show podman images information"
        run: podman images --digests

      # https://cri-o.io/
      - name: Install cri-o
        run: |
          set -Eeuxo pipefail

          sudo apt-get update
          sudo apt-get install -y software-properties-common curl

          curl -fsSL https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/Release.key | \
            sudo gpg --dearmor --batch --yes -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

          echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/ /" | \
            sudo tee /etc/apt/sources.list.d/kubernetes.list

          curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/Release.key | \
            sudo gpg --dearmor --batch --yes -o /etc/apt/keyrings/cri-o-apt-keyring.gpg

          echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/ /" | \
            sudo tee /etc/apt/sources.list.d/cri-o.list

          sudo apt-get update
          sudo apt-get install -y cri-o kubelet kubeadm kubectl
          
          sudo cp ci/cached-builds/crio.conf /etc/crio/crio.conf.d/
          
          sudo systemctl start crio.service

          sudo swapoff -a
          sudo modprobe br_netfilter
          sudo sysctl -w net.ipv4.ip_forward=1
          
          # https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm
          sudo kubeadm init --cri-socket=/var/run/crio/crio.sock
          
          mkdir -p $HOME/.kube
          sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $(id -u):$(id -g) $HOME/.kube/config
        env:
          CRIO_VERSION: v1.30
          KUBERNETES_VERSION: v1.30

      - name: Untaint
        run: kubectl taint nodes --all node-role.kubernetes.io/control-plane-

      - name: Install pod network
        run: kubectl apply -f https://github.com/flannel-io/flannel/releases/download/$FLANNEL_VERSION/kube-flannel.yml
        env:
          FLANNEL_VERSION: v0.26.1

      - name: Want for pods to be running
        run: |
          kubectl wait pods --all --all-namespaces --for=condition=Ready --timeout=300s

      - name: "Show crio images information"
        run: sudo crictl images
          


#        env:
#          KIND_EXPERIMENTAL_PROVIDER: podman
      #
      #      - name: "pull_request|schedule: resolve target if Trivy scan should run"
#        id: resolve-target
#        if: ${{ fromJson(inputs.github).event_name == 'pull_request' || fromJson(inputs.github).event_name == 'schedule' }}
#        env:
#          EVENT_NAME: ${{ fromJson(inputs.github).event_name }}
#          HAS_TRIVY_LABEL: ${{ contains(fromJson(inputs.github).event.pull_request.labels.*.name, 'trivy-scan') }}
#          FS_SCAN_FOLDER: ${{ fromJson(env.TRIVY_SCAN_FS_JSON)[inputs.target] }}
#        run: |
#          if [[ "$EVENT_NAME" == "pull_request" && "$HAS_TRIVY_LABEL" == "true" ]]; then
#            if [[ -n "$FS_SCAN_FOLDER" ]]; then
#              TARGET="$FS_SCAN_FOLDER"
#              TYPE="fs"
#            else
#              TARGET="ghcr.io/${{ github.repository }}/workbench-images:${{ inputs.target }}-${{ github.sha }}"
#              TYPE="image"
#            fi
#          elif [[ "$EVENT_NAME" == "schedule" ]]; then
#            if [[ -n "$FS_SCAN_FOLDER" ]]; then
#              TARGET="$FS_SCAN_FOLDER"
#              TYPE="fs"
#            else
#              TARGET="ghcr.io/${{ github.repository }}/workbench-images:${{ inputs.target }}-${{ github.ref_name }}_${{ github.sha }}"
#              TYPE="image"
#            fi
#          fi
#
#          if [[ -n "$TARGET" ]]; then
#            echo "target=$TARGET" >> $GITHUB_OUTPUT
#            echo "type=$TYPE" >> $GITHUB_OUTPUT
#            echo "Trivy scan will run on $TARGET ($TYPE)"
#          else
#            echo "Trivy scan won't run"
#          fi
#
#      - name: Run Trivy vulnerability scanner
#        if: ${{ steps.resolve-target.outputs.target }}
#        run: |
#          TRIVY_VERSION=0.53.0
#          REPORT_FOLDER=${{ github.workspace }}/report
#          REPORT_FILE=trivy-report.md
#          REPORT_TEMPLATE=trivy-markdown.tpl
#
#          mkdir -p $REPORT_FOLDER
#          cp ci/$REPORT_TEMPLATE $REPORT_FOLDER
#
#          SCAN_TARGET=${{ steps.resolve-target.outputs.target }}
#          SCAN_TYPE=${{ steps.resolve-target.outputs.type }}
#          echo "Scanning $SCAN_TARGET ($SCAN_TYPE)"
#
#          if [[ "$SCAN_TYPE" == "image" ]]; then
#            SCAN_ARGS="--image-src podman --podman-host /var/run/podman/podman.sock"
#            PODMAN_ARGS="-v ${PODMAN_SOCK}:/var/run/podman/podman.sock"
#          elif [[ "$SCAN_TYPE" == "fs" ]]; then
#            WORKSPACE_FOLDER="/workspace"
#            SCAN_TARGET="$WORKSPACE_FOLDER/$SCAN_TARGET"
#            PODMAN_ARGS="-v ${{ github.workspace }}:$WORKSPACE_FOLDER"
#          fi
#
#          # have trivy access podman socket,
#          # https://github.com/aquasecurity/trivy/issues/580#issuecomment-666423279
#          podman run --rm \
#              $PODMAN_ARGS \
#              -v ${REPORT_FOLDER}:/report \
#              docker.io/aquasec/trivy:$TRIVY_VERSION \
#                $SCAN_TYPE \
#                $SCAN_ARGS \
#                --scanners vuln --ignore-unfixed \
#                --exit-code 0 --timeout 30m \
#                --format template --template "@/report/$REPORT_TEMPLATE" -o /report/$REPORT_FILE \
#                $SCAN_TARGET
#
#          cat $REPORT_FOLDER/$REPORT_FILE >> $GITHUB_STEP_SUMMARY

      # https://playwright.dev/docs/ci
      # https://playwright.dev/docs/docker
      # we leave little free disk space after we mount LVM for podman storage
      # not enough to install playwright; running playwright in podman uses the space we have
      - name: Run Playwright tests
        if: ${{ fromJson(inputs.github).event_name == 'pull_request' && contains(inputs.target, 'codeserver') }}
        # --ipc=host because Microsoft says so in Playwright docs
        # --net=host because testcontainers connects to the Reaper container's exposed port
        # we need to pass through the relevant environment variables
        #  DEBUG configures nodejs debuggers, sets different verbosity as needed
        #  CI=true is set on every CI nowadays
        #  PODMAN_SOCK should be mounted to /var/run/docker.sock, other likely mounting locations may not exist (mkdir -p)
        #  TEST_TARGET is the workbench image the test will run
        # --volume(s) let us access docker socket and not clobber host's node_modules
        run: |
          podman run \
            --interactive --rm \
            --ipc=host \
            --net=host \
            --env "CI=true" \
            --env "NPM_CONFIG_fund=false" \
            --env "DEBUG=testcontainers:*" \
            --env "PODMAN_SOCK=/var/run/docker.sock" \
            --env "TEST_TARGET" \
            --volume ${PODMAN_SOCK}:/var/run/docker.sock \
            --volume ${PWD}:/mnt \
            --volume /mnt/node_modules \
            mcr.microsoft.com/playwright:v1.48.1-noble \
            /bin/bash <<EOF
              set -Eeuxo pipefail
              cd /mnt
              npm install -g pnpm && pnpm install
              pnpm exec playwright test
              exit 0
          EOF
        working-directory: tests/browser
        env:
          TEST_TARGET: "ghcr.io/${{ github.repository }}/workbench-images:${{ inputs.target }}-${{ github.sha }}"
      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() && fromJson(inputs.github).event_name == 'pull_request' && contains(inputs.target, 'codeserver') }}
        with:
          name: "${{ inputs.target }}_playwright-report"
          path: tests/browser/playwright-report/
          retention-days: 30

      - run: df -h
        if: "${{ !cancelled() }}"
